{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MSSQL connection details\n",
    "# HOME MACHINE = DESKTOP-CMTGLLQ\n",
    "# CORP MACHINE = JM-DKT-033\n",
    "connection_string = 'DRIVER={SQL Server};SERVER=JM-DKT-033;DATABASE=JLEARN;trusted_connection=YES'\n",
    "\n",
    "# Connect to the database\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for the Medallion architecture\n",
    "base_dir = \"warehouse\"\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "def create_folders():\n",
    "    layers = ['raw', 'prep', 'mart']\n",
    "    for directory in layers:\n",
    "        if not os.path.exists(base_dir + \"/\" + directory):\n",
    "            os.makedirs(base_dir + \"/\" + directory)\n",
    "\n",
    "create_folders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PREPARING DATABASE\n",
    "# def clean_db():\n",
    "#     schemas = ['raw','prep','mart']\n",
    "#     tables = ['user_courses','users','courses']\n",
    "#     def create_schema(schema):\n",
    "#         query = f\"select schema_id('{schema}');\"\n",
    "#         cursor.execute(query)\n",
    "#         res = tuple(cursor.fetchall()[0])[0]\n",
    "#         if(res == None):\n",
    "#             query = f\"create schema {schema};\"\n",
    "#             cursor.execute(query)\n",
    "#             print(f\"{schema} schema created\")\n",
    "#             conn.commit()\n",
    "\n",
    "#     def drop_tables(schema, table):\n",
    "#         query = f\"DROP TABLE IF EXISTS {schema}.{table};\"\n",
    "#         cursor.execute(query)\n",
    "#         conn.commit()\n",
    "\n",
    "#     def create_users_table(schema):\n",
    "#         query = f\"\"\"CREATE TABLE {schema}.users (\n",
    "#             UserID       INT            PRIMARY KEY,\n",
    "#             UserName     NVARCHAR(100),\n",
    "#             FullName     NVARCHAR(100),\n",
    "#             Email        NVARCHAR(255) UNIQUE,\n",
    "#             PasswordHash NVARCHAR(255),\n",
    "#             Role         NVARCHAR(50)   DEFAULT 'employee',\n",
    "#             RegisteredAt DATETIME       DEFAULT GETDATE()\n",
    "#         )\"\"\"\n",
    "#         cursor.execute(query)\n",
    "#         conn.commit()\n",
    "\n",
    "#     def create_courses_table(schema):\n",
    "#         query = f\"\"\"CREATE TABLE {schema}.courses (\n",
    "#             course_id           INT            PRIMARY KEY,\n",
    "#             course_title        NVARCHAR(100),\n",
    "#             num_subscribers     INT,\n",
    "#             num_reviews         SMALLINT,\n",
    "#             num_lectures        SMALLINT,\n",
    "#             level               NVARCHAR(50),\n",
    "#             content_duration    FLOAT,\n",
    "#             published_timestamp NVARCHAR(50),\n",
    "#             subject             NVARCHAR(50)\n",
    "#         )\"\"\"\n",
    "#         cursor.execute(query)\n",
    "#         conn.commit()\n",
    "\n",
    "#     def create_user_courses_table(schema):\n",
    "#         query = f\"\"\"CREATE TABLE {schema}.user_courses (\n",
    "#             user_course_id INT       PRIMARY KEY,\n",
    "#             user_id        INT,\n",
    "#             course_id      INT,\n",
    "#             status         VARCHAR(50),\n",
    "#             progress       DECIMAL(5, 2) DEFAULT 0.00,\n",
    "#             enrolled_at    DATETIME      DEFAULT GETDATE(),\n",
    "#             completed_at   DATETIME,\n",
    "#             score          DECIMAL(5, 2),\n",
    "#             CONSTRAINT FK_user_courses_users FOREIGN KEY (user_id) REFERENCES raw.users(UserID),\n",
    "#             CONSTRAINT FK_user_courses_courses FOREIGN KEY (course_id) REFERENCES raw.courses(course_id)\n",
    "#         )\"\"\"\n",
    "#         cursor.execute(query)\n",
    "#         conn.commit()\n",
    "\n",
    "#     for schema in schemas:\n",
    "#         create_schema(schema)\n",
    "#         for table in tables:\n",
    "#             drop_tables(schema, table)\n",
    "\n",
    "#     create_users_table('raw')\n",
    "#     create_courses_table('raw')\n",
    "#     create_user_courses_table('raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data using pyodbc and convert to pandas DataFrame\n",
    "def fetch_data(query, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    columns = [column[0] for column in cursor.description]\n",
    "    data = cursor.fetchall()\n",
    "    df = pd.DataFrame([tuple(row) for row in data], columns=columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to save data to both CSV and SQL Server\n",
    "\n",
    "def save_data(df, table_name, stage):\n",
    "\n",
    "    # Save to CSV\n",
    "    if \"PasswordHash\" in df.columns:\n",
    "        df = df.drop(\"PasswordHash\", axis=1) \n",
    "        \n",
    "    file_path = os.path.join(base_dir + \"\\\\\" +stage, f'{table_name}.csv')\n",
    "    # {datetime.now().strftime(\"%Y%m%d_%H%M%S\")}\n",
    "    # print(file_path)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    # Save to SQL Server\n",
    "\n",
    "    # df = df.applymap(lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if isinstance(x, pd.Timestamp) else x)\n",
    "    # df = df.applymap(lambda x: float(x) if isinstance(x, Decimal) else x)\n",
    "    # df = df.applymap(lambda x: None if isinstance(x, pd.notna) else x)\n",
    "    # df = df.applymap(lambda x: None if (isinstance(x, Decimal) and (x.is_nan() or x == Decimal('NaN'))) else float(x) if isinstance(x, Decimal) else x)\n",
    "    # df = df.replace(\"'\",\"\", regex=True)\n",
    "\n",
    "    \n",
    "    # conn_str = f\"INSERT INTO {schema}.{table_name} ({', '.join(df.columns)}) VALUES \"\n",
    "    # values = ', '.join([str(tuple(row)) for row in df.values])\n",
    "    # query = conn_str + values\n",
    "    # print(values)\n",
    "    # with conn.cursor() as cursor:\n",
    "    #     cursor.execute(query)\n",
    "    #     conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T FORGET TO CREATE RESPECTIVE TABLES IN EACH SCHEMA\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1. RAW Layer: Raw data ingestion from tables\n",
    "# ----------------------------------------------\n",
    "# Ingest raw data from SQL Server\n",
    "def raw_ingestion():\n",
    "    user_query = \"SELECT * FROM users\"\n",
    "    course_query = \"SELECT * FROM courses\"\n",
    "    user_courses_query = \"SELECT * FROM user_courses\"\n",
    "    \n",
    "    # Fetching raw data\n",
    "    users_df = fetch_data(user_query, conn)\n",
    "    courses_df = fetch_data(course_query, conn)\n",
    "    user_courses_df = fetch_data(user_courses_query, conn)\n",
    "    \n",
    "    # Save raw data\n",
    "    save_data(users_df, 'users','raw')\n",
    "    save_data(courses_df, 'courses', 'raw')\n",
    "    save_data(user_courses_df, 'user_courses', 'raw')\n",
    "\n",
    "# clean_db()\n",
    "raw_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 2. PREP Layer: Cleansing and Enrichment\n",
    "# ----------------------------------------------\n",
    "def prep_transformation():\n",
    "    # Clean and join the data (Enrichment)\n",
    "    raw_courses_df = pd.read_csv(\"warehouse/raw/courses.csv\")\n",
    "    raw_user_courses_df = pd.read_csv(\"warehouse/raw/user_courses.csv\")\n",
    "    raw_users_df = pd.read_csv(\"warehouse/raw/users.csv\")\n",
    "    \n",
    "\n",
    "    raw_users_df = raw_users_df.dropna()\n",
    "    raw_courses_df = raw_courses_df.dropna()\n",
    "    raw_user_courses_df = raw_user_courses_df.dropna()\n",
    "\n",
    "    save_data(raw_users_df, 'users', 'prep')\n",
    "    save_data(raw_courses_df, 'courses', 'prep')\n",
    "    save_data(raw_user_courses_df, 'user_courses', 'prep')\n",
    "    \n",
    "\n",
    "prep_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 3. MART Layer: Aggregation and Analysis\n",
    "# ----------------------------------------------\n",
    "raw_ml_input = None\n",
    "\n",
    "def mart_transformation():\n",
    "    global raw_ml_input\n",
    "    # Aggregate course completion statistics\n",
    "    # gold_query = \"\"\"\n",
    "    # SELECT c.course_title, COUNT(uc.user_course_id) as num_users, AVG(uc.progress) as avg_progress\n",
    "    # FROM user_courses uc\n",
    "    # JOIN courses c ON uc.course_id = c.course_id\n",
    "    # WHERE uc.progress = 100\n",
    "    # GROUP BY c.course_title\n",
    "    # \"\"\"\n",
    "\n",
    "    prep_courses_df = pd.read_csv(\"warehouse/prep/courses.csv\")\n",
    "    prep_user_courses_df = pd.read_csv(\"warehouse/prep/user_courses.csv\")\n",
    "    prep_users_df = pd.read_csv(\"warehouse/prep/users.csv\")\n",
    "\n",
    "    merged_df = pd.merge(prep_users_df, prep_user_courses_df, left_on='UserID', right_on='user_id', how='inner')\n",
    "\n",
    "    # Step 2: Merge the result with courses on course_id\n",
    "    final_merged_df = pd.merge(merged_df, prep_courses_df, left_on='course_id', right_on='course_id', how='inner')\n",
    "    final_merged_df = final_merged_df.drop(\"user_id\", axis=1)\n",
    "\n",
    "    final_merged_df['enrolled_at'] = pd.to_datetime(final_merged_df['enrolled_at'])\n",
    "\n",
    "    raw_ml_input = final_merged_df.sort_values(by='enrolled_at').groupby('UserID').agg({\n",
    "        'course_id': list,\n",
    "        'score': list\n",
    "    }).reset_index()\n",
    "\n",
    "    save_data(final_merged_df, 'report', 'mart')\n",
    "\n",
    "mart_transformation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data - UserIDs, course sequences and scores\n",
    "import collections\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = float(len(dictionary) + 1)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "courses_data = pd.read_csv(\"csv/data.csv\")\n",
    "vocab_size = len(courses_data) + 1\n",
    "\n",
    "courses_dict, reverse_course_dict = (build_dataset(courses_data[\"course_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_courses_df = pd.read_csv(\"warehouse/prep/courses.csv\")\n",
    "prep_user_courses_df = pd.read_csv(\"warehouse/prep/user_courses.csv\")\n",
    "prep_users_df = pd.read_csv(\"warehouse/prep/users.csv\")\n",
    "\n",
    "merged_df = pd.merge(prep_users_df, prep_user_courses_df, left_on='UserID', right_on='user_id', how='inner')\n",
    "\n",
    "# Step 2: Merge the result with courses on course_id\n",
    "final_merged_df = pd.merge(merged_df, prep_courses_df, left_on='course_id', right_on='course_id', how='inner')\n",
    "final_merged_df = final_merged_df.drop(\"user_id\", axis=1)\n",
    "\n",
    "final_merged_df['enrolled_at'] = pd.to_datetime(final_merged_df['enrolled_at'])\n",
    "\n",
    "data = final_merged_df.sort_values(by='enrolled_at').groupby('UserID').agg({\n",
    "    'course_id': list,\n",
    "    'score': list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "data['course_id'] = data['course_id'].apply(lambda x: [courses_dict[course] for course in x])\n",
    "for courses in data['course_id']:\n",
    "    for i in range(1, len(courses)):\n",
    "        X.append(courses[:i])       # All courses up to i\n",
    "        y.append(courses[i])        # Next course (course_id)\n",
    "\n",
    "# Pad sequences to ensure uniform input shape\n",
    "max_seq_length = max([len(seq) for seq in X])\n",
    "X_padded = pad_sequences(X, maxlen=max_seq_length)\n",
    "\n",
    "X_padded = np.array(X_padded)\n",
    "y_encoded = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "embedding_dim = 8  # Embedding dimension\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(64),\n",
    "    Dense(vocab_size, activation='softmax')  # Output probabilities for course IDs\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 4.3951 - val_accuracy: 0.0000e+00 - val_loss: 4.3944\n",
      "Epoch 2/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0764 - loss: 4.3736 - val_accuracy: 0.0000e+00 - val_loss: 4.3917\n",
      "Epoch 3/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1111 - loss: 4.3419 - val_accuracy: 0.0000e+00 - val_loss: 4.3930\n",
      "Epoch 4/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0324 - loss: 4.2575 - val_accuracy: 0.0270 - val_loss: 4.4531\n",
      "Epoch 5/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0058 - loss: 4.1454 - val_accuracy: 0.0270 - val_loss: 4.5546\n",
      "Epoch 6/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0214 - loss: 3.9741 - val_accuracy: 0.0270 - val_loss: 4.5517\n",
      "Epoch 7/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0604 - loss: 3.9109 - val_accuracy: 0.0541 - val_loss: 4.5375\n",
      "Epoch 8/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1112 - loss: 3.8101 - val_accuracy: 0.0270 - val_loss: 4.5262\n",
      "Epoch 9/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1589 - loss: 3.7483 - val_accuracy: 0.0270 - val_loss: 4.5256\n",
      "Epoch 10/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1422 - loss: 3.5911 - val_accuracy: 0.0270 - val_loss: 4.5144\n",
      "Epoch 11/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1834 - loss: 3.4064 - val_accuracy: 0.0270 - val_loss: 4.5149\n",
      "Epoch 12/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2257 - loss: 3.2762 - val_accuracy: 0.0270 - val_loss: 4.5163\n",
      "Epoch 13/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2502 - loss: 3.0483 - val_accuracy: 0.0270 - val_loss: 4.5454\n",
      "Epoch 14/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3322 - loss: 2.8284 - val_accuracy: 0.0270 - val_loss: 4.5810\n",
      "Epoch 15/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3888 - loss: 2.6990 - val_accuracy: 0.0270 - val_loss: 4.6131\n",
      "Epoch 16/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3939 - loss: 2.6347 - val_accuracy: 0.0270 - val_loss: 4.6876\n",
      "Epoch 17/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3753 - loss: 2.4898 - val_accuracy: 0.0270 - val_loss: 4.7332\n",
      "Epoch 18/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4134 - loss: 2.2419 - val_accuracy: 0.0270 - val_loss: 4.8041\n",
      "Epoch 19/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4621 - loss: 2.0739 - val_accuracy: 0.0270 - val_loss: 4.8348\n",
      "Epoch 20/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3710 - loss: 2.2095 - val_accuracy: 0.0270 - val_loss: 4.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x260e5487c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=2, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ml_artifacts/lstm_v1.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
