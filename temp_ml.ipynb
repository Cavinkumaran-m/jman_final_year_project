{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data - UserIDs, course sequences and scores\n",
    "import collections\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = float(len(dictionary) + 1)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "courses_data = pd.read_csv(\"csv/data.csv\")\n",
    "vocab_size = len(courses_data) + 1\n",
    "\n",
    "courses_dict, reverse_course_dict = (build_dataset(courses_data[\"course_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_courses_df = pd.read_csv(\"warehouse/prep/courses.csv\")\n",
    "prep_user_courses_df = pd.read_csv(\"warehouse/prep/user_courses.csv\")\n",
    "prep_users_df = pd.read_csv(\"warehouse/prep/users.csv\")\n",
    "\n",
    "merged_df = pd.merge(prep_users_df, prep_user_courses_df, left_on='UserID', right_on='user_id', how='inner')\n",
    "\n",
    "# Step 2: Merge the result with courses on course_id\n",
    "final_merged_df = pd.merge(merged_df, prep_courses_df, left_on='course_id', right_on='course_id', how='inner')\n",
    "final_merged_df = final_merged_df.drop(\"user_id\", axis=1)\n",
    "\n",
    "final_merged_df['enrolled_at'] = pd.to_datetime(final_merged_df['enrolled_at'])\n",
    "\n",
    "data = final_merged_df.sort_values(by='enrolled_at').groupby('UserID').agg({\n",
    "    'course_id': list,\n",
    "    'score': list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "data['course_id'] = data['course_id'].apply(lambda x: [courses_dict[course] for course in x])\n",
    "for courses in data['course_id']:\n",
    "    for i in range(1, len(courses)):\n",
    "        X.append(courses[:i])       # All courses up to i\n",
    "        y.append(courses[i])        # Next course (course_id)\n",
    "\n",
    "# Pad sequences to ensure uniform input shape\n",
    "max_seq_length = max([len(seq) for seq in X])\n",
    "X_padded = pad_sequences(X, maxlen=max_seq_length)\n",
    "\n",
    "X_padded = np.array(X_padded)\n",
    "y_encoded = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "embedding_dim = 8  # Embedding dimension\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(64),\n",
    "    Dense(vocab_size, activation='softmax')  # Output probabilities for course IDs\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.0150 - loss: 4.3936 - val_accuracy: 0.0000e+00 - val_loss: 4.3977\n",
      "Epoch 2/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0814 - loss: 4.3735 - val_accuracy: 0.0000e+00 - val_loss: 4.4061\n",
      "Epoch 3/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0347 - loss: 4.3248 - val_accuracy: 0.0000e+00 - val_loss: 4.4515\n",
      "Epoch 4/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0168 - loss: 4.2362 - val_accuracy: 0.0000e+00 - val_loss: 4.6402\n",
      "Epoch 5/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0309 - loss: 4.1487 - val_accuracy: 0.0000e+00 - val_loss: 4.9467\n",
      "Epoch 6/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0186 - loss: 4.0577 - val_accuracy: 0.0000e+00 - val_loss: 5.1616\n",
      "Epoch 7/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0348 - loss: 3.9504 - val_accuracy: 0.0000e+00 - val_loss: 5.3734\n",
      "Epoch 8/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1330 - loss: 3.8494 - val_accuracy: 0.0000e+00 - val_loss: 5.5941\n",
      "Epoch 9/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1300 - loss: 3.7609 - val_accuracy: 0.0000e+00 - val_loss: 5.7241\n",
      "Epoch 10/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1736 - loss: 3.5739 - val_accuracy: 0.0000e+00 - val_loss: 5.8947\n",
      "Epoch 11/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1537 - loss: 3.4523 - val_accuracy: 0.0000e+00 - val_loss: 6.0632\n",
      "Epoch 12/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2338 - loss: 3.2579 - val_accuracy: 0.0000e+00 - val_loss: 6.1974\n",
      "Epoch 13/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1704 - loss: 3.2581 - val_accuracy: 0.0000e+00 - val_loss: 6.2720\n",
      "Epoch 14/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2173 - loss: 3.0557 - val_accuracy: 0.0000e+00 - val_loss: 6.4324\n",
      "Epoch 15/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2826 - loss: 2.8269 - val_accuracy: 0.0000e+00 - val_loss: 6.5809\n",
      "Epoch 16/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3179 - loss: 2.7420 - val_accuracy: 0.0000e+00 - val_loss: 6.6800\n",
      "Epoch 17/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4249 - loss: 2.6108 - val_accuracy: 0.0000e+00 - val_loss: 6.8334\n",
      "Epoch 18/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3653 - loss: 2.5420 - val_accuracy: 0.0270 - val_loss: 6.8970\n",
      "Epoch 19/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4411 - loss: 2.3655 - val_accuracy: 0.0000e+00 - val_loss: 7.0475\n",
      "Epoch 20/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3587 - loss: 2.3891 - val_accuracy: 0.0000e+00 - val_loss: 7.1529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=2, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step\n",
      "[18, 14, 9]\n",
      "[[65 61 75 56 80 68 32 69 41 10  0 64 48  2 57 42 33 39 50 26 51 70 21 31\n",
      "  35 43 59 36 54 23 15 60 25 49 38 44 67 27 55 29 30 34 40 13 28  8 22 79\n",
      "  77 53  7 12 37 24 46 17 73 66 16  6 72 20 45  3 78 47 63 58 19 71 52  5\n",
      "  62  1 74  4 76 11  9 14 18]]\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[12, 2]])  # Example sequence (same shape as training data)\n",
    "\n",
    "# Predict probabilities for each course ID\n",
    "predictions = model.predict(new_data)\n",
    "sorted = np.argsort(predictions)\n",
    "\n",
    "res = []\n",
    "for i in range(1, 4):\n",
    "    res.append(sorted[0][i * -1])\n",
    "print(res)\n",
    "print(sorted)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
